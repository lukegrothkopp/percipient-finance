# app.py ‚Äî Percipient Finance 

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
import re
import numpy as np
import os
import json
from pathlib import Path

# =========================
# Page configuration & logo
# =========================
REPO_DIR = Path(__file__).parent
LOGO_PATH = REPO_DIR / "assets" / "header_logo.png"

def page_icon_value():
    return str(LOGO_PATH) if LOGO_PATH.exists() else "üìà"

st.set_page_config(
    page_title="Grothko Consulting's Percipipent Finance Insights",
    page_icon=page_icon_value(),
    layout="wide",
    initial_sidebar_state="expanded"
)

# =========================
# Custom CSS
# =========================
st.markdown("""
<style>
.main-header {
    font-size: 2.5rem;
    font-weight: bold;
    color: #1f77b4;
    text-align: center;
    margin-bottom: 2rem;
}
.metric-card {
    background-color: #f0f2f6;
    padding: 1rem;
    border-radius: 0.5rem;
    margin: 0.5rem 0;
}
</style>
""", unsafe_allow_html=True)

# =========================
# API key loader
# =========================
def get_openai_api_key():
    """
    Precedence:
      1) st.secrets["OPENAI_API_KEY"]
      2) st.secrets["openai"]["api_key"]
      3) os.environ["OPENAI_API_KEY"]
    """
    key = None
    try:
        key = st.secrets["OPENAI_API_KEY"]
    except Exception:
        try:
            key = st.secrets["openai"]["api_key"]
        except Exception:
            key = os.environ.get("OPENAI_API_KEY")
    return key

# =========================
# Session State
# =========================
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None
if 'retriever' not in st.session_state:
    st.session_state.retriever = None
if 'llm' not in st.session_state:
    st.session_state.llm = None
if 'data_summary' not in st.session_state:
    st.session_state.data_summary = ""
if 'page' not in st.session_state:
    st.session_state.page = "Dashboard"
if 'finance_mapping' not in st.session_state:
    st.session_state.finance_mapping = {}
if 'mapping_confirmed' not in st.session_state:
    st.session_state.mapping_confirmed = False
if 'budget_df' not in st.session_state:
    st.session_state.budget_df = None

# ===========================================
# Finance canonical fields (Suggestion 0)
# ===========================================
FINANCE_CANONICAL_FIELDS = {
    "date": ["date","order_date","invoice_date","transaction_date","purchase_date"],
    "customer_id": ["customer","customer_id","cust_id","account_id"],
    "vendor_id": ["vendor","vendor_id","supplier_id"],
    "sku": ["sku","product_id","item_id"],
    "quantity": ["quantity","qty","units","units_sold","unit_sold"],
    "revenue": ["revenue","sales","net_revenue","net_sales","amount"],
    "cogs": ["cogs","cost_of_goods_sold","cost"],
    "op_ex": ["opex","operating_expense","expense","operating_expenses"],
    "invoice_due_date": ["due_date","invoice_due","terms_due"],
    "invoice_paid_date": ["paid_date","payment_date","cash_date"],
    "ar_amount": ["ar","accounts_receivable","ar_amount"],
    "ap_amount": ["ap","accounts_payable","ap_amount"],
    "inventory_value": ["inventory","inventory_value","stock_value"],
    "cash_in": ["cash_in","collections","receipts","inflows","cash_receipts"],
    "cash_out": ["cash_out","disbursements","payments","outflows","cash_disbursements"],

    # SaaS-ish / contracts:
    "contract_start": ["contract_start","start_date"],
    "contract_end": ["contract_end","end_date","renewal_date"],
    "mrr": ["mrr","monthly_recurring_revenue"],
    "arr": ["arr","annual_recurring_revenue"],
    "logo_id": ["logo_id","account_id","customer_id"],
    "plan": ["plan","package","tier"],
    "is_churned": ["churn","is_churned","churned"],
    "is_expansion": ["expansion","upsell","add_on"],
    "discount": ["discount","discount_pct","discount_percent"],
    "s&m_expense": ["sales_marketing","sales_and_marketing","sm_expense"],

    # Extended keys:
    "unit_price": ["unit_price","price","avg_price","net_price","average_price"],
    "period": ["period","month","year_month","fiscal_period","ym","yyyy_mm"],
    "account": ["account","gl_account","budget_account","pnl_account"],
    "amount": ["amount","value","usd_amount"]
}
MAPPING_STORE = REPO_DIR / ".if_mapping.json"

# Attempt to load persisted mapping
try:
    if MAPPING_STORE.exists():
        persisted = json.loads(MAPPING_STORE.read_text())
        if isinstance(persisted, dict):
            st.session_state.finance_mapping.update(persisted)
except Exception:
    pass

# =========================
# Helpers
# =========================
def _norm(s: str) -> str:
    return re.sub(r'[^a-z0-9]', '', str(s).lower())

def _find_col(df: pd.DataFrame, name: str):
    """Fuzzy/semantic column matcher."""
    def alias_set(s: str):
        alts = {s}
        if 'percentage' in s:
            alts.update({s.replace('percentage','pct'), s.replace('percentage','percent')})
        if 'percent' in s:
            alts.update({s.replace('percent','pct'), s.replace('percent','percentage')})
        if 'perc' in s:
            alts.update({s.replace('perc','pct'), s.replace('perc','percent'), s.replace('perc','percentage')})
        if 'pct' in s:
            alts.update({s.replace('pct','percent'), s.replace('pct','percentage'), s.replace('pct','perc')})
        return alts

    semantic_aliases = {
        'price':        ['unit_price','price','avg_price','averageprice'],
        'revenue':      ['net_revenue','revenue','sales','totalrevenue','netsales'],
        'quantity':     ['units','quantity','qty','unitssold','units_sold'],
        'qty':          ['units','quantity','qty','unitssold','units_sold'],
        'date':         ['purchase_date','order_date','date','purchasedate','orderdate'],
        'customer':     ['customer_id','customerid','cust_id','custid','id'],
        'discount':     ['discount_pct','discount','discountpercent','discountpercentage','pctdiscount'],
        'department':   ['department','dept'],
        'labor_cost':   ['labor_cost','labour_cost','payroll','wages'],
        'hours':        ['hours','hrs'],
        'period':       ['period','month','fiscal_period','ym','yyyy_mm'],
    }
    norm2actual = {_norm(c): c for c in df.columns}
    target = _norm(name)
    target_alts = alias_set(target)

    # exact/alias
    for ta in [target, *list(target_alts)]:
        if ta in norm2actual: return norm2actual[ta]
    # contains
    for ta in [target, *list(target_alts)]:
        for cn, actual in norm2actual.items():
            if ta in cn or cn in ta:
                return actual
    # semantic
    concept_map = {
        'price': ['price','unitprice','avgprice','averageprice'],
        'revenue': ['revenue','netsales','sales','totalrevenue'],
        'quantity': ['quantity','qty','units'],
        'date': ['date','orderdate','purchasedate'],
        'department': ['department','dept'],
        'labor_cost': ['labor','wages','payroll'],
        'hours': ['hours','hrs'],
        'period': ['period','month','fiscal'],
    }
    hits=[]
    for k,hints in concept_map.items():
        if target==k or any(h in target for h in hints): hits.append(k)
    seen=set(); hits=[h for h in hits if not (h in seen or seen.add(h))]
    for concept in hits:
        if concept in semantic_aliases:
            for cand in semantic_aliases[concept]:
                cn=_norm(cand)
                if cn in norm2actual: return norm2actual[cn]
                for nn, actual in norm2actual.items():
                    if cn in nn or nn in cn:
                        return actual
    # last resort
    for nn, actual in norm2actual.items():
        if any(tok in nn for tok in [target,*list(target_alts)]):
            return actual
    return None

def _first_datetime_col(df: pd.DataFrame):
    for c in df.columns:
        if pd.api.types.is_datetime64_any_dtype(df[c]):
            return c
        try:
            pd.to_datetime(df[c], errors='raise')
            return c
        except Exception:
            continue
    return None

# =========================
# Data summary for RAG
# =========================
def create_data_summary(df: pd.DataFrame) -> str:
    parts=[]
    parts.append(f"Dataset Overview: The dataset contains {len(df)} records and {df.shape[1]} columns.")
    parts.append(f"Columns: {', '.join(df.columns.tolist())}")
    numeric=df.select_dtypes(include=['float64','int64','float32','int32']).columns
    if len(numeric)>0:
        parts.append("\nNumeric Columns Analysis:")
        for col in numeric:
            try:
                parts.append(f"\n{col}:")
                parts.append(f"  - Mean: {df[col].mean():.2f}")
                parts.append(f"  - Median: {df[col].median():.2f}")
                parts.append(f"  - Std Dev: {df[col].std():.2f}")
                parts.append(f"  - Min: {df[col].min():.2f}")
                parts.append(f"  - Max: {df[col].max():.2f}")
            except Exception: pass
    cats=df.select_dtypes(include=['object']).columns
    if len(cats)>0:
        parts.append("\nCategorical Columns Analysis:")
        for col in cats:
            try:
                uniq=df[col].nunique()
                parts.append(f"\n{col}:")
                parts.append(f"  - Unique values: {uniq}")
                if uniq<=10:
                    vc=df[col].value_counts()
                    parts.append(f"  - Distribution: {vc.to_dict()}")
            except Exception: pass
    missing=df.isnull().sum()
    if missing.sum()>0:
        parts.append("\nMissing Data:")
        for col in missing[missing>0].index:
            parts.append(f"  - {col}: {missing[col]} missing values")
    completeness=(1 - df.isnull().sum().sum() / max(1,(df.shape[0]*df.shape[1]))) * 100
    parts.append("\nKey Insights:")
    parts.append(f"  - Total rows: {len(df)}")
    parts.append(f"  - Data completeness: {completeness:.1f}%")
    return "\n".join(parts)

# =========================
# RAG setup
# =========================
def setup_rag_system(df: pd.DataFrame, api_key: str) -> bool:
    try:
        os.environ["OPENAI_API_KEY"]=api_key
        summary=create_data_summary(df)
        st.session_state.data_summary=summary
        data_insights=f"""
{summary}

Sample Data Records (first 20):
{df.head(20).to_string()}

Column Data Types and Info:
{df.dtypes.to_string()}
"""
        splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)
        chunks=splitter.split_text(data_insights)
        embeddings=OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore=FAISS.from_texts(chunks, embeddings)
        retriever=vectorstore.as_retriever(search_kwargs={"k":3})
        llm=ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
        st.session_state.vectorstore=vectorstore
        st.session_state.retriever=retriever
        st.session_state.llm=llm
        return True
    except Exception as e:
        st.error(f"Error setting up AI system: {e}")
        return False

def answer_with_rag(question: str):
    retriever=st.session_state.retriever
    source_docs=[]; context=""
    if retriever is not None:
        try:
            source_docs=retriever.invoke(question)
            context="\n\n".join(getattr(doc,"page_content",str(doc)) for doc in source_docs)
        except Exception as e:
            st.error(f"Retrieval error: {e}")
            source_docs=[]; context=""
    history_tail=st.session_state.chat_history[-6:]
    history_text="\n".join([f"{m['role'].capitalize()}: {m['content']}" for m in history_tail])
    system_text=(
        "You are a helpful business data analyst. Use the provided context (from the uploaded CSV) "
        "to answer the user's question. If the answer is not in the context, say you don't have enough "
        "information and suggest what to compute or inspect."
    )
    prompt=(f"{system_text}\n\nChat history:\n{history_text}\n\nContext:\n{context}\n\n"
            f"User question: {question}\n\nAnswer clearly and concisely:")
    try:
        resp=st.session_state.llm.invoke(prompt)
        answer=getattr(resp,"content",str(resp))
        return answer, source_docs
    except Exception as e:
        return f"Error generating response: {e}", source_docs

# =========================
# Structured Analytics Engine
# =========================
def _resolve_condition_phrase(df: pd.DataFrame, phrase: str):
    p_raw = phrase.strip()
    p = p_raw.lower()

    p = re.sub(r'^(customers?|users?|people)\s+(who|that|which)\s+', '', p).strip()

    eq_match = re.search(r'(=| is | equals )\s*([^\s].+)$', p)
    explicit_value = None
    if eq_match:
        explicit_value = eq_match.group(2).strip().strip('"\'').lower()
        p = p[:eq_match.start()].strip()

    falsy_hints = (" do not ", " don't ", " not ", " no ", " non ", " without ")
    polarity = "truthy"
    if any(h in f" {p} " for h in falsy_hints):
        polarity = "falsy"

    intent_priority = [
        ("recommend", ["recommend", "recommended", "recommends", "would recommend", "nps", "promoter"]),
        ("coupon",    ["coupon", "promo", "code", "discount code", "coupon used", "used coupon"]),
        ("subscription", ["subscription", "subscriber", "member", "membership", "loyalty"]),
        ("repeat",    ["repeat", "returning", "again"]),
        ("gift",      ["gift", "gifting", "gifted"]),
        ("feedback",  ["feedback", "rating", "review", "satisfaction"]),
    ]

    def _try_concept(concept: str):
        col = _find_col(df, concept)
        return col

    for concept, tokens in intent_priority:
        if any(t in p for t in tokens):
            col = _try_concept(concept)
            if col:
                return col, explicit_value, polarity

    col = _find_col(df, p)
    if col:
        return col, explicit_value, polarity

    tokens = re.findall(r'[a-z0-9_%]+', p)
    tokens_sorted = sorted(tokens, key=len, reverse=True)
    for tok in tokens_sorted:
        col = _find_col(df, tok)
        if col:
            return col, explicit_value, polarity

    for i in range(len(tokens_sorted) - 1):
        bg = tokens_sorted[i] + " " + tokens_sorted[i + 1]
        col = _find_col(df, bg)
        if col:
            return col, explicit_value, polarity

    return None, explicit_value, polarity

def _normalize_boolish(series: pd.Series) -> pd.Series:
    s = series.astype(str).str.lower().str.strip()
    truthy = {'1','true','t','yes','y','recommended','recommend','member','used'}
    falsy  = {'0','false','f','no','n','notrecommended','dontrecommend','do_not_recommend','unused'}

    def map_val(v):
        if v in truthy: return '___truthy___'
        if v in falsy:  return '___falsy___'
        return v

    return s.map(map_val)

def handle_analytics_query(question: str, df: pd.DataFrame):
    """
    Detect and execute common analytics directly on the dataframe.
    Returns (handled: bool, message: str).
    """
    q = question.lower().strip()

    # --- Correlation matrix on numeric columns ---
    if "correlation matrix" in q or "corr matrix" in q or re.search(r"\bheat\s*map\b.*corr", q):
        num_df = df.select_dtypes(include=[np.number])
        if num_df.shape[1] < 2:
            return True, "I don't have at least two numeric columns to compute a correlation matrix."
        corr = num_df.corr()
        try:
            fig = px.imshow(corr, text_auto=True, aspect="auto",
                            title="Correlation Matrix (numeric columns)",
                            color_continuous_scale='RdBu')
            st.plotly_chart(fig, use_container_width=True)
        except Exception:
            pass
        return True, f"Computed a correlation matrix across {num_df.shape[1]} numeric columns."

    # --- Hours vs labor_cost by department ---
    if ("hours" in q and ("labor" in q or "labour" in q)) and ("department" in q or "across" in q):
        hours_col = _find_col(df, "hours")
        labor_col = _find_col(df, "labor_cost") or _find_col(df, "labour_cost")
        dept_col  = _find_col(df, "department")
        missing = [name for name, col in [("hours", hours_col), ("labor_cost", labor_col), ("department", dept_col)] if not col or col not in df.columns]
        if missing:
            msg = ("I couldn't find the required columns: " + ", ".join(missing) +
                   ". Load a staffing dataset (e.g., 'staffing_payroll_events.csv') "
                   "with columns: hours, labor_cost, department.")
            return True, msg

        x = pd.to_numeric(df[hours_col], errors='coerce')
        y = pd.to_numeric(df[labor_col], errors='coerce')
        valid = x.notna() & y.notna()
        overall = x[valid].corr(y[valid], method='pearson') if valid.sum() >= 3 else np.nan

        out_rows = []
        for dept, g in df.loc[valid, [dept_col, hours_col, labor_col]].groupby(dept_col):
            gx = pd.to_numeric(g[hours_col], errors='coerce')
            gy = pd.to_numeric(g[labor_col], errors='coerce')
            mask = gx.notna() & gy.notna()
            r = gx[mask].corr(gy[mask], method='pearson') if mask.sum() >= 3 else np.nan
            out_rows.append({"department": dept, "n": int(mask.sum()), "pearson_r": r})

        res = pd.DataFrame(out_rows).sort_values(["n","pearson_r"], ascending=[False, False])
        st.subheader("Hours vs Labor Cost ‚Äî Department Correlations")
        st.dataframe(res)

        try:
            top = res.dropna(subset=["pearson_r"]).head(1)
            if not top.empty:
                top_dept = top.iloc[0]["department"]
                g = df[(df[dept_col] == top_dept) & valid]
                fig = px.scatter(g, x=hours_col, y=labor_col, title=f"Scatter: {hours_col} vs {labor_col} ‚Äî {top_dept}")
                st.plotly_chart(fig, use_container_width=True)
        except Exception:
            pass

        msg = (f"Overall Pearson r between **{hours_col}** and **{labor_col}**: "
               f"**{overall:.3f}**" if pd.notna(overall) else
               "Not enough overlapping numeric values to compute overall correlation.")
        return True, msg

    # --- Correlation between X and Y ---
    m = re.search(r'correlat\w*\s+.*\bbetween\b\s+(.+?)\s+\b(and|&)\b\s+(.+)', q)
    if m:
        raw_x = m.group(1); raw_y = m.group(3)
        col_x = _find_col(df, raw_x); col_y = _find_col(df, raw_y)
        if not col_x or not col_y:
            return True, (f"I couldn't match both columns.\nMatched X: `{col_x or 'None'}` | "
                          f"Matched Y: `{col_y or 'None'}`.\nColumns: {list(df.columns)}")
        x = pd.to_numeric(df[col_x], errors='coerce'); y = pd.to_numeric(df[col_y], errors='coerce')
        valid = x.notna() & y.notna()
        if valid.sum() < 3:
            return True, f"Not enough overlapping numeric values to compute correlation between `{col_x}` and `{col_y}`."
        r = x[valid].corr(y[valid], method='pearson')
        msg = f"**Pearson r** between `{col_x}` and `{col_y}`: **{r:.3f}** (n={valid.sum()})"
        st.write(msg)
        try:
            fig = px.scatter(pd.DataFrame({col_x:x[valid], col_y:y[valid]}), x=col_x, y=col_y,
                             title=f"Scatter: {col_x} vs {col_y}")
            st.plotly_chart(fig, use_container_width=True)
        except Exception:
            pass
        return True, msg

    # --- Explicit Pearson request ---
    m2 = re.search(r'(pearson|corr(?!elation)\b).*?(between|for)\s+[\'"]?([\w %_]+)[\'"]?\s+(and|,)\s+[\'"]?([\w %_]+)[\'"]?', q)
    if m2:
        raw_x = m2.group(3); raw_y = m2.group(5)
        col_x = _find_col(df, raw_x); col_y = _find_col(df, raw_y)
        if not col_x or not col_y:
            return True, (f"I couldn't match both columns.\nMatched X: `{col_x or 'None'}` | "
                          f"Matched Y: `{col_y or 'None'}`.\nColumns: {list(df.columns)}")
        x = pd.to_numeric(df[col_x], errors='coerce'); y = pd.to_numeric(df[col_y], errors='coerce')
        valid = x.notna() & y.notna()
        if valid.sum() < 3:
            return True, f"Not enough overlapping numeric values to compute correlation between `{col_x}` and `{col_y}`."
        r = x[valid].corr(y[valid], method='pearson')
        msg = f"**Pearson r** between `{col_x}` and `{col_y}`: **{r:.3f}** (n={valid.sum()})"
        st.write(msg)
        try:
            fig = px.scatter(pd.DataFrame({col_x:x[valid], col_y:y[valid]}), x=col_x, y=col_y,
                             title=f"Scatter: {col_x} vs {col_y}")
            st.plotly_chart(fig, use_container_width=True)
        except Exception:
            pass
        return True, msg

    # --- Conditional average ---
    m_avg = re.search(
        r'\b(average|avg|mean)\b\s+(of\s+|for\s+)?(?P<metric>[\w %_]+?)\s+(among|for|where)\s+(?P<condphrase>.+)$',
        q
    )
    if m_avg:
        raw_metric = m_avg.group('metric').strip()
        cond_phrase = m_avg.group('condphrase').strip()

        metric_col = _find_col(df, raw_metric)
        if not metric_col:
            return True, (f"I couldn't resolve the requested metric column.\n"
                          f"Metric: `{raw_metric}`\nColumns: {list(df.columns)}")

        cond_col, explicit_val, polarity = _resolve_condition_phrase(df, cond_phrase)
        if not cond_col:
            return True, (f"I couldn't resolve the condition column from: `{cond_phrase}`.\n"
                          f"Columns: {list(df.columns)}")

        raw_series = df[cond_col]
        norm_series = _normalize_boolish(raw_series)

        if explicit_val:
            target = explicit_val.lower().strip()
            cond_mask = norm_series.eq(target) | raw_series.astype(str).str.lower().str.strip().eq(target)
        else:
            flag = '___truthy___' if polarity == 'truthy' else '___falsy___'
            cond_mask = norm_series.eq(flag)

            if not cond_mask.any():
                if raw_series.dtype == bool:
                    cond_mask = raw_series.fillna(False) if polarity == "truthy" else ~raw_series.fillna(False)
                else:
                    try:
                        num_series = pd.to_numeric(raw_series, errors='coerce').fillna(0)
                        cond_mask = (num_series != 0) if polarity == "truthy" else (num_series == 0)
                    except Exception:
                        pass

        metric_vals = pd.to_numeric(df.loc[cond_mask, metric_col], errors='coerce').dropna()
        n = metric_vals.shape[0]
        if n == 0:
            pretty_val = (f"= {explicit_val}" if explicit_val else (" is truthy" if polarity=="truthy" else " is falsy"))
            return True, (f"No matching rows for **{cond_col}** {pretty_val}.")

        mean_val = metric_vals.mean()
        pretty_val = (f"= {explicit_val}" if explicit_val else (" is truthy" if polarity=="truthy" else " is falsy"))
        msg = (f"**Average {metric_col}** for rows where **{cond_col}** {pretty_val}: "
               f"**{mean_val:.2f}** (n={n})")
        st.write(msg)

        try:
            fig = px.histogram(metric_vals, nbins=20, title=f"Distribution of {metric_col} (filtered)")
            st.plotly_chart(fig, use_container_width=True)
        except Exception:
            pass

        return True, msg

    # --- "Top 3 insights" ---
    if re.search(r'\btop\s*3\s*insight', q):
        insights=[]
        num_df=df.select_dtypes(include=[np.number])
        if num_df.shape[1]>=2:
            corr=num_df.corr().abs()
            np.fill_diagonal(corr.values,0)
            max_pair=divmod(corr.values.argmax(), corr.shape[1])
            a,b=num_df.columns[max_pair[0]], num_df.columns[max_pair[1]]
            insights.append(f"Strongest numeric relationship: **{a}** vs **{b}** (|r| ‚âà **{corr.values.max():.3f}**).")
        cat_cols=df.select_dtypes(include=['object']).columns.tolist()
        if cat_cols:
            vc=df[cat_cols[0]].value_counts(dropna=True).head(3)
            insights.append(f"Top categories in **{cat_cols[0]}**: " + ", ".join([f'{k} ({v})' for k,v in vc.items()]))
        if num_df.shape[1]>=1 and cat_cols:
            grp=df.groupby(cat_cols[0])[num_df.columns[0]].mean().sort_values(ascending=False).head(3)
            insights.append(f"Highest average **{num_df.columns[0]}** by **{cat_cols[0]}**: " +
                            ", ".join([f"{idx} ({val:.2f})" for idx,val in grp.items()]))
        if not insights:
            return True, "I couldn't derive quick insights‚Äîdataset might lack numeric/categorical variety."
        msg="### Top 3 Insights\n" + "\n".join([f"{i+1}. {insights[i]}" for i in range(min(3,len(insights)))])
        return True, msg

    # --- "Trends or patterns" ---
    if re.search(r'\btrends?\b|\bpatterns?\b', q):
        bullets=[]
        num_df=df.select_dtypes(include=[np.number])
        date_col=_first_datetime_col(df)
        if date_col:
            try:
                df2=df.copy()
                df2[date_col]=pd.to_datetime(df2[date_col], errors='coerce')
                if num_df.shape[1]>=1:
                    ycol=num_df.columns[0]
                    ts=df2.dropna(subset=[date_col, ycol]).sort_values(date_col)
                    if len(ts)>=3:
                        ts['__period']=ts[date_col].dt.to_period('M').dt.to_timestamp()
                        agg=ts.groupby('__period')[ycol].mean()
                        direction="increasing" if agg.iloc[-1]>agg.iloc[0] else "decreasing"
                        bullets.append(f"**Time trend** in **{ycol}**: appears {direction} from first to last observed month.")
                        try:
                            fig=px.line(agg.reset_index(), x='__period', y=ycol, title=f"Trend of {ycol} over time")
                            st.plotly_chart(fig, use_container_width=True)
                        except Exception: pass
            except Exception: pass
        if num_df.shape[1]>=2:
            corr=num_df.corr(); corr_abs=corr.abs()
            np.fill_diagonal(corr_abs.values,0)
            max_pair=divmod(corr_abs.values.argmax(), corr_abs.shape[1])
            a,b=num_df.columns[max_pair[0]], num_df.columns[max_pair[1]]
            bullets.append(f"**Strongest numeric relationship**: {a} vs {b} (|r| ‚âà {corr_abs.values.max():.3f}).")
        if not bullets:
            bullets.append("No clear trends/patterns detected automatically. Try asking about specific columns or groups.")
        msg="### Detected Trends & Patterns\n" + "\n".join([f"- {b}" for b in bullets])
        return True, msg

    return False, ""

# =========================
# Finance KPI Engine helpers
# =========================
def _col(name):  # convenience
    return st.session_state.finance_mapping.get(name)

def _parse_dates(df, cols):
    for c in cols:
        if c and c in df.columns:
            df[c] = pd.to_datetime(df[c], errors="coerce")
    return df

def compute_dso(df):
    inv = _col("date") or _col("invoice_date")
    paid = _col("invoice_paid_date")
    ar_amt = _col("ar_amount")
    rev = _col("revenue")
    if not inv or not (paid or ar_amt or rev):
        return None, "Missing required fields for DSO."
    df = df.copy()
    _parse_dates(df, [inv, paid])
    if ar_amt in df.columns and rev in df.columns:
        avg_ar = df[ar_amt].mean(skipna=True)
        total_rev = df[rev].sum(skipna=True)
        dso = (avg_ar / (total_rev/365.0)) if total_rev else np.nan
        return dso, None
    if paid in df.columns:
        mask = df[paid].notna() & df[inv].notna()
        if mask.sum()==0: return None, "No paid vs. invoice dates to compute realized DSO."
        durs = (df.loc[mask, paid] - df.loc[mask, inv]).dt.days
        return durs.mean(), None
    return None, "Insufficient data for DSO."

def compute_dpo(df):
    inv = _col("date")
    paid = _col("invoice_paid_date")
    ap_amt = _col("ap_amount")
    cogs = _col("cogs")
    if ap_amt in df.columns and cogs in df.columns:
        avg_ap = df[ap_amt].mean(skipna=True)
        total_cogs = df[cogs].sum(skipna=True)
        dpo = (avg_ap / (total_cogs/365.0)) if total_cogs else np.nan
        return dpo, None
    if inv and paid and inv in df.columns and paid in df.columns:
        _parse_dates(df, [inv, paid])
        mask = df[paid].notna() & df[inv].notna()
        if mask.sum()==0: return None, "No paid vs. bill dates to compute realized DPO."
        durs = (df.loc[mask, paid] - df.loc[mask, inv]).dt.days
        return durs.mean(), None
    return None, "Insufficient data for DPO."

def compute_dio(df):
    inv_val = _col("inventory_value")
    cogs = _col("cogs")
    if inv_val in df.columns and cogs in df.columns:
        avg_inv = df[inv_val].mean(skipna=True)
        total_cogs = df[cogs].sum(skipna=True)
        dio = (avg_inv / (total_cogs/365.0)) if total_cogs else np.nan
        return dio, None
    return None, "Insufficient data for DIO."

def compute_ccc(df):
    dso, _ = compute_dso(df)
    dpo, _ = compute_dpo(df)
    dio, _ = compute_dio(df)
    if None not in (dso,dpo,dio):
        return (dso + dio - dpo), None
    return None, "Insufficient data for full CCC."

def gross_margin(df):
    rev = _col("revenue"); cogs = _col("cogs")
    if rev in df.columns and cogs in df.columns:
        total_rev = df[rev].sum(skipna=True)
        gm = (total_rev - df[cogs].sum(skipna=True))
        return gm, (gm/total_rev*100 if total_rev else np.nan)
    return None, None

def contribution_margin_by(df, by):
    rev = _col("revenue"); cogs = _col("cogs")
    if not (rev in df.columns and cogs in df.columns and by in df.columns):
        return None
    gp = df.groupby(by).agg({rev:"sum", cogs:"sum"})
    gp["gross_profit"] = gp[rev]-gp[cogs]
    gp["gm_pct"] = np.where(gp[rev]!=0, gp["gross_profit"]/gp[rev]*100, np.nan)
    return gp.sort_values("gross_profit", ascending=False)

def vendor_pareto(df):
    v = _col("vendor_id"); ap = _col("ap_amount")
    if v in df.columns and ap in df.columns:
        s = df.groupby(v)[ap].sum().sort_values(ascending=False)
        s_pct = s.cumsum()/s.sum()*100 if s.sum()!=0 else s.cumsum()
        return s, s_pct
    return None, None

def churn_and_retention(df):
    lid = _col("logo_id"); churn = _col("is_churned")
    if lid in df.columns and churn in df.columns:
        total = df[lid].nunique()
        churned = df.loc[df[churn]==True, lid].nunique()
        grr = (1 - churned/total)*100 if total else np.nan
        return {"logos": total, "churned": churned, "GRR_pct": grr}
    return None

def ltv_simple(arpu, gross_margin_pct, months):
    return arpu * (gross_margin_pct/100.0) * months

def cac_payback_months(cac, arpu, gm_pct):
    monthly_gp = arpu * (gm_pct/100.0)
    return cac / monthly_gp if monthly_gp else np.nan

def magic_number(delta_arr_quarter, prior_q_sm_expense):
    return (delta_arr_quarter*4.0) / prior_q_sm_expense if prior_q_sm_expense else np.nan

# =========================
# Optional-but-powerful features
# =========================
# (Patched) 13-week cash view
def weekly_13_week_cash(df, opening_cash=0.0, min_cash_threshold=0.0):
    dt = _col("date") or _find_col(df, "date")
    ci = _col("cash_in")
    co = _col("cash_out")

    if not dt or dt not in df.columns:
        return None, "Map a valid 'date' column to compute 13-week cash."

    w = df.copy()
    _parse_dates(w, [dt])

    # Fallbacks: if user didn't map, try to infer
    if not ci or ci not in w.columns:
        w["__cash_in__"] = 0.0
        ci = "__cash_in__"
    if not co or co not in w.columns:
        guess = _find_col(w, "labor_cost")
        if guess and guess in w.columns:
            co = guess
        else:
            w["__cash_out__"] = 0.0
            co = "__cash_out__"

    # Coerce to numeric
    w[ci] = pd.to_numeric(w[ci], errors="coerce").fillna(0.0)
    w[co] = pd.to_numeric(w[co], errors="coerce").fillna(0.0)

    if (w[ci].sum() == 0) and (w[co].sum() == 0):
        return None, "No numeric cash_in/cash_out values found. Map 'cash_out' to a numeric column (e.g., labor_cost) or load a cash ledger."

    # Week bucketing (Mon-start)
    try:
        w["week"] = w[dt].dt.to_period("W-MON").apply(lambda p: p.start_time)
    except Exception:
        w["week"] = w[dt] - w[dt].dt.weekday.astype("timedelta64[D]")

    agg = w.groupby("week").agg(
        cash_in=(ci, "sum"),
        cash_out=(co, "sum"),
    ).sort_index()

    agg["net"] = agg["cash_in"] - agg["cash_out"]
    agg["ending_cash"] = float(opening_cash) + agg["net"].cumsum()
    agg["flag_low_cash"] = agg["ending_cash"] < float(min_cash_threshold)

    # Show context: last 3 past weeks + upcoming weeks, max 13 rows
    today_monday = pd.Timestamp.today().normalize() - pd.to_timedelta(pd.Timestamp.today().weekday(), unit="D")
    future = agg[agg.index >= today_monday]
    past = agg[agg.index < today_monday].tail(3)
    out = pd.concat([past, future]).head(13)

    return out, None

# BvA + forecast error
def join_budget_actuals(actuals_df, budget_df):
    per = _col("period"); acc = _col("account"); amt = _col("amount")
    dt  = _col("date"); rev = _col("revenue"); cogs = _col("cogs"); opx = _col("op_ex")

    if budget_df is None:
        return None, "Upload a Budget CSV."
    if per not in budget_df.columns or acc not in budget_df.columns or amt not in budget_df.columns:
        return None, "Budget CSV must have mapped 'period', 'account', and 'amount'."

    a = actuals_df.copy()
    if per not in a.columns:
        if dt in a.columns:
            a = _parse_dates(a, [dt])
            a[per] = a[dt].dt.to_period("M").astype(str)
        else:
            return None, "Map either 'period' or 'date' in actuals."

    pieces = []
    if rev in a.columns: pieces.append(a.groupby(per)[rev].sum().rename("amount").reset_index().assign(account="Revenue"))
    if cogs in a.columns: pieces.append(a.groupby(per)[cogs].sum().rename("amount").reset_index().assign(account="COGS"))
    if opx in a.columns: pieces.append(a.groupby(per)[opx].sum().rename("amount").reset_index().assign(account="Opex"))
    if not pieces:
        return None, "Need at least one of revenue, cogs, or op_ex in actuals to compute BvA."
    act = pd.concat(pieces, ignore_index=True)
    act = act.rename(columns={per:"period"})
    act["account"] = act["account"].astype(str)

    bud = budget_df[[per, acc, amt]].copy()
    bud = bud.rename(columns={per:"period", acc:"account", amt:"budget"})
    bud["account"] = bud["account"].astype(str)

    m = pd.merge(act, bud, on=["period","account"], how="left")
    m["variance"] = m["amount"] - m["budget"]
    m["var_pct"] = np.where(m["budget"]!=0, m["variance"]/m["budget"]*100, np.nan)
    return m, None

def forecast_error_metrics(df_period_vs, group_by_account=True):
    df = df_period_vs.dropna(subset=["amount","budget"]).copy()
    df["abs_pct"] = np.where(df["budget"]!=0, (df["amount"]-df["budget"]).abs()/df["budget"], np.nan)
    df["abs_err"] = (df["amount"]-df["budget"]).abs()
    df["abs_act"] = df["amount"].abs()
    if group_by_account:
        out = df.groupby("account").agg(
            MAPE=("abs_pct","mean"),
            WAPE=("abs_err","sum")
        )
        denom = df.groupby("account")["abs_act"].sum()
        out["WAPE"] = out["WAPE"] / denom.replace(0,np.nan)
        out["MAPE"] = out["MAPE"]*100; out["WAPE"]=out["WAPE"]*100
        return out.sort_values("WAPE", ascending=False)
    else:
        mape = df["abs_pct"].mean()*100
        wape = df["abs_err"].sum()/df["abs_act"].sum()*100 if df["abs_act"].sum()!=0 else np.nan
        return pd.DataFrame({"MAPE":[mape],"WAPE":[wape]})

# PVM waterfall (needs sku/quantity/unit_price/period in dataset to run)
def pvm_decomposition(df, period_a, period_b):
    sku = _col("sku"); qty = _col("quantity"); price = _col("unit_price") or _find_col(df,"price"); per = _col("period")
    if not all([sku, qty, price, per]) or any(c not in df.columns for c in [sku, qty, price, per]):
        return None, "Map 'sku', 'quantity', 'unit_price' (or 'price'), and 'period' to run PVM."
    d = df.copy()
    d = d[d[per].astype(str).isin([str(period_a), str(period_b)])]
    if d.empty: return None, "Selected periods not found."

    agg = d.groupby([per, sku]).agg(Q=(qty,"sum"), P=(price,"mean")).reset_index()
    a = agg[agg[per].astype(str)==str(period_a)].set_index(sku)[["Q","P"]].rename(columns={"Q":"Qa","P":"Pa"})
    b = agg[agg[per].astype(str)==str(period_b)].set_index(sku)[["Q","P"]].rename(columns={"Q":"Qb","P":"Pb"})
    j = a.join(b, how="outer").fillna(0.0)

    RevA = (j["Qa"]*j["Pa"]).sum()
    RevB = (j["Qb"]*j["Pb"]).sum()

    price_effect  = (j["Qb"]*(j["Pb"]-j["Pa"])).sum()
    volume_effect = (j["Pa"]*(j["Qb"]-j["Qa"])).sum()
    mix_effect    = ((j["Qb"]-j["Qa"])*(j["Pb"]-j["Pa"])).sum()

    out = pd.DataFrame({
        "Component":["Revenue A","Price","Volume","Mix","Revenue B"],
        "Value":[RevA, price_effect, volume_effect, mix_effect, RevB]
    })
    out["Delta"] = out["Value"].diff()
    return out, None

# Cohorts & NRR (sponsor-as-logo model if you expand by month)
def cohort_table_nrr(df):
    lid = _col("logo_id") or _col("customer_id")
    dt  = _col("date")
    mrr = _col("mrr") or _col("arr")
    if not (lid and dt and mrr) or any(c not in df.columns for c in [lid, dt, mrr]):
        return None, None, "Map 'logo_id' (or 'customer_id'), 'date', and 'mrr' (or 'arr')."
    x = df[[lid, dt, mrr]].copy()
    _parse_dates(x, [dt])
    x["ym"] = x[dt].dt.to_period("M").astype(str)
    first = x.groupby(lid)["ym"].min().rename("cohort")
    x = x.join(first, on=lid)
    cube = x.groupby(["cohort","ym"])[mrr].sum().unstack(fill_value=0.0)
    first_col = cube.columns[0] if len(cube.columns) else None
    if first_col is None:
        return cube, cube, None
    nrr = cube.divide(cube[first_col].replace(0,np.nan), axis=0)
    return cube, nrr, None

def overall_retention_curves(df):
    lid = _col("logo_id") or _col("customer_id")
    dt  = _col("date")
    mrr = _col("mrr") or _col("arr")
    if not (lid and dt and mrr):
        return None, None, "Map 'logo_id'/'customer_id', 'date', and 'mrr'/'arr'."
    d = df[[lid, dt, mrr]].copy()
    _parse_dates(d, [dt])
    d["ym"] = d[dt].dt.to_period("M").astype(str)
    by = d.groupby(["ym", lid])[mrr].sum().reset_index()
    pivot = by.pivot(index=lid, columns="ym", values=mrr).fillna(0.0)
    months = sorted(pivot.columns)
    grr, nrr = [], []
    for i in range(1, len(months)):
        prev, cur = months[i-1], months[i]
        m_prev = pivot[prev]; m_cur = pivot[cur]
        base_prev = m_prev.sum()
        grr_base = np.minimum(m_prev, m_cur).sum() / (base_prev if base_prev!=0 else np.nan)
        nrr_base = (m_cur.sum() / (base_prev if base_prev!=0 else np.nan))
        grr.append({"month": cur, "GRR": grr_base*100})
        nrr.append({"month": cur, "NRR": nrr_base*100})
    return pd.DataFrame(grr), pd.DataFrame(nrr), None

# =========================
# Multi-file unify helpers
# =========================
def _ensure_period(df, date_col="date"):
    df = df.copy()
    if "period" not in df.columns and date_col in df.columns:
        df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
        df["period"] = df[date_col].dt.to_period("M").astype(str)
    return df

def expand_sponsorships_monthly(spon_df):
    # expects: logo_id, contract_start, contract_end, mrr
    spon_df = spon_df.copy()
    if "contract_start" not in spon_df.columns or "contract_end" not in spon_df.columns:
        return pd.DataFrame(columns=["date","period","revenue","cogs","op_ex"])
    spon_df["contract_start"] = pd.to_datetime(spon_df["contract_start"], errors="coerce")
    spon_df["contract_end"]   = pd.to_datetime(spon_df["contract_end"], errors="coerce")
    rows = []
    for _, r in spon_df.iterrows():
        if pd.isna(r["contract_start"]) or pd.isna(r["contract_end"]): 
            continue
        start_p = r["contract_start"].to_period("M")
        end_p   = r["contract_end"].to_period("M")
        ym = pd.period_range(start_p, end_p, freq="M")
        for p in ym.astype(str):
            rows.append({"date": pd.Period(p).to_timestamp("M"), "period": p,
                         "logo_id": r.get("logo_id"), "mrr": r.get("mrr", 0), "revenue": r.get("mrr", 0)})
    return pd.DataFrame(rows)

def assemble_unified_ledger(file_objs):
    """Build a monthly ledger (date, period, revenue, cogs, op_ex) by recognizing schemas."""
    parts = []
    for f in file_objs:
        try:
            df = pd.read_csv(f)
        except Exception:
            continue
        cols = set(c.lower() for c in df.columns)

        # actuals_transactions.csv
        if {"date","period","revenue","cogs","op_ex"}.issubset(cols):
            df = _ensure_period(df, "date")
            parts.append(df[["date","period","revenue","cogs","op_ex"]])

        # department_opex_monthly.csv
        elif {"department","opex_amount"}.issubset(cols) and "period" in cols:
            out = df.groupby("period", as_index=False)["opex_amount"].sum().rename(columns={"opex_amount":"op_ex"})
            out["date"] = pd.PeriodIndex(out["period"], freq="M").to_timestamp("M")
            out["revenue"] = 0; out["cogs"] = 0
            parts.append(out[["date","period","revenue","cogs","op_ex"]])

        # sponsorships.csv -> monthly revenue
        elif {"logo_id","contract_start","contract_end","mrr"}.issubset(cols):
            out = expand_sponsorships_monthly(df)
            if not out.empty:
                out["cogs"] = 0; out["op_ex"] = 0
                parts.append(out[["date","period","revenue","cogs","op_ex"]])

        # pos_food_beverage.csv
        elif {"event_id","date","outlet","sales_amount","cogs_amount"}.issubset(cols):
            df = _ensure_period(df, "date")
            g = df.groupby("period", as_index=False).agg(revenue=("sales_amount","sum"), cogs=("cogs_amount","sum"))
            g["date"] = pd.PeriodIndex(g["period"], freq="M").to_timestamp("M"); g["op_ex"]=0
            parts.append(g[["date","period","revenue","cogs","op_ex"]])

        # merchandise_sales.csv
        elif {"event_id","date","stand","sales_amount","cogs_amount"}.issubset(cols):
            df = _ensure_period(df, "date")
            g = df.groupby("period", as_index=False).agg(revenue=("sales_amount","sum"), cogs=("cogs_amount","sum"))
            g["date"] = pd.PeriodIndex(g["period"], freq="M").to_timestamp("M"); g["op_ex"]=0
            parts.append(g[["date","period","revenue","cogs","op_ex"]])

        # staffing_payroll_events.csv (treat as opex)
        elif {"event_id","date","department","hours","labor_cost"}.issubset(cols):
            df = _ensure_period(df, "date")
            g = df.groupby("period", as_index=False).agg(op_ex=("labor_cost","sum"))
            g["date"] = pd.PeriodIndex(g["period"], freq="M").to_timestamp("M")
            g["revenue"]=0; g["cogs"]=0
            parts.append(g[["date","period","revenue","cogs","op_ex"]])

        else:
            # Fallback: ignore (or could attempt auto-map)
            pass

    if not parts:
        return None
    uni = pd.concat(parts, ignore_index=True)
    uni = uni.groupby(["period"], as_index=False).agg({
        "revenue":"sum","cogs":"sum","op_ex":"sum"
    })
    uni["date"] = pd.PeriodIndex(uni["period"], freq="M").to_timestamp("M")
    return uni[["date","period","revenue","cogs","op_ex"]]

# =========================
# Sidebar
# =========================
with st.sidebar:
    if LOGO_PATH.exists():
        st.sidebar.image(str(LOGO_PATH), width=150)
    else:
        st.sidebar.markdown("### GROTHKO CONSULTING")

    st.title("Navigation")

    pages = ["Dashboard", "Finance KPIs", "Data Analysis", "AI Assistant", "Visualizations"]
    current_index = pages.index(st.session_state.page) if st.session_state.page in pages else 0
    selection = st.radio("Select Page:", pages, index=current_index)
    st.session_state.page = selection

    # API Key
    st.subheader("üóùÔ∏è API Configuration")
    api_key = get_openai_api_key()
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key
        st.success("üí™ API key verified")
    else:
        st.info("‚ÑπÔ∏è Store your key in Streamlit Secrets as `OPENAI_API_KEY` (or `openai.api_key`).")

    # Multi-file uploader (unified ledger)
    st.subheader("üóÑÔ∏è Data Upload")
    uploaded_files = st.file_uploader(
        "Upload one or more datasets (CSV)", type=['csv'], accept_multiple_files=True,
        help="Upload actuals, POS, merch, sponsorships, department opex, staffing, etc."
    )

    # Budget upload (for BvA)
    with st.expander("üì• Budget Upload (for BvA)", expanded=False):
        budget_file = st.file_uploader("Upload Budget CSV", type=["csv"], key="budget_csv")
        if budget_file is not None:
            try:
                st.session_state.budget_df = pd.read_csv(budget_file)
                st.success(f"Loaded budget with {len(st.session_state.budget_df):,} rows.")
            except Exception as e:
                st.error(f"Failed to read budget CSV: {e}")

    # Build unified ledger if any files were provided
    if uploaded_files:
        unified = assemble_unified_ledger(uploaded_files)
        if unified is not None and not unified.empty:
            st.session_state.df = unified
            st.session_state.data_loaded = True
            st.success(f"Unified dataset built: {unified.shape[0]} rows √ó {unified.shape[1]} columns")
            # Set up RAG once
            if st.session_state.vectorstore is None and api_key:
                with st.spinner("Setting up AI system..."):
                    if setup_rag_system(unified, api_key):
                        st.success("ü¶æ AI system ready!")
        else:
            st.info("Uploaded files recognized, but nothing unified. Try loading a known schema (e.g., actuals, POS, merch, sponsorships, department opex, staffing).")

    # Finance Field Mapping (persisted)
    if st.session_state.data_loaded:
        with st.expander("üîß Finance Field Mapping (optional but recommended)", expanded=False):
            df = st.session_state.df
            for canon, guesses in FINANCE_CANONICAL_FIELDS.items():
                persisted = st.session_state.finance_mapping.get(canon)
                default_guess = None
                if not persisted:
                    for c in df.columns:
                        normc = c.lower().replace(" ","").replace("_","")
                        if normc in [g.replace("_","") for g in guesses]:
                            default_guess = c
                            break
                options = ["(none)"] + df.columns.tolist()
                preselect = 0
                if persisted and persisted in df.columns:
                    preselect = options.index(persisted)
                elif default_guess and default_guess in df.columns:
                    preselect = options.index(default_guess)
                choice = st.selectbox(
                    f"Map **{canon}** to:",
                    options,
                    index=preselect,
                    key=f"map_{canon}"
                )
                st.session_state.finance_mapping[canon] = None if choice=="(none)" else choice

            if st.button("‚úÖ Confirm Finance Mapping"):
                st.session_state.mapping_confirmed = True
                try:
                    MAPPING_STORE.write_text(json.dumps(st.session_state.finance_mapping, indent=2))
                    st.success("Mapping saved for future sessions.")
                except Exception as e:
                    st.warning(f"Could not persist mapping: {e}")

# =========================
# Header
# =========================
st.markdown('<h1 class="main-header">Percipient AI Financial Analyst</h1>', unsafe_allow_html=True)

# =========================
# Page Routing
# =========================
page = st.session_state.get("page", "Dashboard")

# ---- Dashboard ----
if page == "Dashboard":
    st.header("üìà BI Dashboard")
    if st.session_state.data_loaded:
        df = st.session_state.df
        c1,c2,c3,c4 = st.columns(4)
        with c1: st.metric(label="Total Records", value=f"{len(df):,}", delta="Active")
        with c2: st.metric(label="Data Columns", value=df.shape[1], delta="Features")
        with c3:
            numeric_cols = df.select_dtypes(include=['float64','int64']).columns
            if len(numeric_cols)>0:
                st.metric(label=f"Avg {numeric_cols[0]}", value=f"{df[numeric_cols[0]].mean():.2f}")
        with c4:
            completeness=(1 - df.isnull().sum().sum() / max(1,(df.shape[0]*df.shape[1]))) * 100
            st.metric(label="Data Quality", value=f"{completeness:.1f}%", delta="Complete")
        st.divider()
        st.subheader("üóÉÔ∏è Data Preview"); st.dataframe(df.head(10), use_container_width=True)
        st.subheader("üí° Quick Statistics"); st.dataframe(df.describe(), use_container_width=True)
    else:
        st.info("üëà Upload one or more CSVs in the sidebar (plus an optional Budget CSV) to get started.")

# ---- Finance KPIs ----
elif page == "Finance KPIs":
    st.header("üíº Finance KPIs")
    if not st.session_state.data_loaded:
        st.warning("Upload at least one dataset first.")
    elif not st.session_state.mapping_confirmed:
        st.info("Open the sidebar ‚ûú Finance Field Mapping and click **Confirm** to enable calculations.")
    else:
        df = st.session_state.df

        # Working capital tiles
        c1, c2, c3, c4 = st.columns(4)
        with c1:
            dso, e = compute_dso(df); st.metric("DSO (days)", f"{dso:.1f}" if dso is not None else "‚Äî")
            if e: st.caption(f"Note: {e}")
        with c2:
            dpo, e = compute_dpo(df); st.metric("DPO (days)", f"{dpo:.1f}" if dpo is not None else "‚Äî")
            if e: st.caption(f"Note: {e}")
        with c3:
            dio, e = compute_dio(df); st.metric("DIO (days)", f"{dio:.1f}" if dio is not None else "‚Äî")
            if e: st.caption(f"Note: {e}")
        with c4:
            ccc, e = compute_ccc(df); st.metric("Cash Conversion Cycle", f"{ccc:.1f}" if ccc is not None else "‚Äî")
            if e: st.caption(f"Note: {e}")

        st.divider()
        # Profitability
        gp, gm_pct = gross_margin(df)
        colA, colB = st.columns(2)
        with colA:
            st.metric("Gross Profit", f"{gp:,.0f}" if gp is not None else "‚Äî")
        with colB:
            st.metric("Gross Margin %", f"{gm_pct:.1f}%" if gm_pct is not None else "‚Äî")

        # Contribution by
        by_choice = st.selectbox("Contribution margin by:", ["(none)","customer_id","sku"])
        if by_choice != "(none)" and _col(by_choice) in st.session_state.df.columns:
            cm = contribution_margin_by(df, _col(by_choice))
            if cm is not None:
                st.subheader(f"Contribution margin by {_col(by_choice)}")
                st.dataframe(cm.head(25))
                try:
                    fig = px.bar(cm.reset_index().head(15), x=_col(by_choice), y="gross_profit", title="Top Contribution")
                    st.plotly_chart(fig, use_container_width=True)
                except Exception:
                    pass

        st.divider()
        # Vendor Pareto
        s, s_pct = vendor_pareto(df)
        if s is not None:
            st.subheader("Vendor Concentration (Pareto)")
            st.dataframe(pd.DataFrame({"AP_Amount": s, "Cum%": s_pct}).head(25))
            try:
                fig = px.line(pd.DataFrame({"rank": range(1, len(s_pct)+1), "cum_pct": s_pct.values}),
                              x="rank", y="cum_pct", title="Vendor Pareto (cum%)")
                st.plotly_chart(fig, use_container_width=True)
            except Exception:
                pass

        # ---------- 13-Week Cash (patched) ----------
        st.divider()
        st.subheader("üóìÔ∏è 13-Week Cash View")
        oc = st.number_input("Opening Cash (starting point)", value=0.0, step=1000.0)
        th = st.number_input("Low-Cash Threshold (warn below)", value=0.0, step=1000.0)
        tbl, err = weekly_13_week_cash(df, opening_cash=oc, min_cash_threshold=th)
        if err:
            st.caption(f"Note: {err}")
        elif tbl is not None:
            st.dataframe(tbl)
            try:
                fig = px.bar(tbl.reset_index(), x="week", y="net", title="Weekly Net Cash (13 weeks)")
                st.plotly_chart(fig, use_container_width=True)
                fig2 = px.line(tbl.reset_index(), x="week", y="ending_cash", title="Ending Cash by Week")
                st.plotly_chart(fig2, use_container_width=True)
            except Exception:
                pass
            if tbl["flag_low_cash"].any():
                st.warning("Weeks below threshold are flagged in the table.")
            else:
                st.success("No low-cash weeks flagged.")

        # ---------- BvA ----------
        st.divider()
        st.subheader("üìä Budget vs. Actuals")
        if st.session_state.budget_df is None:
            st.caption("Upload a Budget CSV in the sidebar to enable BvA.")
        else:
            bva, err = join_budget_actuals(df, st.session_state.budget_df)
            if err:
                st.caption(f"Note: {err}")
            else:
                st.dataframe(bva.head(50))
                try:
                    fig = px.bar(bva, x="period", y="variance", color="account", title="Variance by Period & Account")
                    st.plotly_chart(fig, use_container_width=True)
                except Exception:
                    pass
                st.markdown("**Forecast error metrics (lower is better):**")
                st.dataframe(forecast_error_metrics(bva))

        # ---------- PVM ----------
        st.divider()
        st.subheader("üí∏ Price-Volume-Mix Waterfall")
        per_col = _col("period")
        if per_col and per_col in df.columns:
            periods = sorted(df[per_col].astype(str).unique())
            if len(periods) >= 2:
                a = st.selectbox("Baseline period", periods, index=max(0,len(periods)-2))
                b = st.selectbox("Compare to period", periods, index=max(0,len(periods)-1))
                res, err = pvm_decomposition(df, a, b)
                if err:
                    st.caption(f"Note: {err}")
                else:
                    st.dataframe(res)
                    try:
                        fig = px.waterfall(res, x="Component", y="Value", title=f"PVM: {a} ‚ûú {b}")
                        st.plotly_chart(fig, use_container_width=True)
                    except Exception:
                        pass
            else:
                st.caption("Need at least two distinct 'period' values for PVM.")
        else:
            st.caption("Map or derive 'period' to enable PVM.")

        # ---------- Cohorts & NRR ----------
        st.divider()
        st.subheader("üß© Cohorts & NRR (Contracts)")
        cohort, nrr_tbl, err = cohort_table_nrr(df)
        if err:
            st.caption(f"Note: {err}")
        else:
            st.markdown("**Cohort MRR (rows=cohort start month, cols=month):**")
            st.dataframe(cohort)
            st.markdown("**Cohort NRR (each cohort normalized to 100% at start):**")
            st.dataframe((nrr_tbl*100).round(1))
            try:
                heat = (nrr_tbl*100).round(1).reset_index().melt(id_vars="cohort", var_name="month", value_name="NRR")
                fig = px.density_heatmap(heat, x="month", y="cohort", z="NRR", title="Cohort NRR Heatmap",
                                         nbinsx=len(heat["month"].unique()))
                st.plotly_chart(fig, use_container_width=True)
            except Exception:
                pass

        grr_curve, nrr_curve, err2 = overall_retention_curves(df)
        if not err2 and grr_curve is not None and nrr_curve is not None and not grr_curve.empty and not nrr_curve.empty:
            c1, c2 = st.columns(2)
            with c1:
                fig1 = px.line(grr_curve, x="month", y="GRR", title="GRR by Month")
                st.plotly_chart(fig1, use_container_width=True)
            with c2:
                fig2 = px.line(nrr_curve, x="month", y="NRR", title="NRR by Month")
                st.plotly_chart(fig2, use_container_width=True)

# ---- Data Analysis ----
elif page == "Data Analysis":
    st.header("üõ∞Ô∏è Advanced Data Analysis")
    if st.session_state.data_loaded:
        df = st.session_state.df
        tab1,tab2,tab3 = st.tabs(["Column Analysis","Missing Data","Correlations"])
        with tab1:
            st.subheader("Column Information")
            col_info = pd.DataFrame({
                'Column': df.columns,
                'Type': df.dtypes,
                'Non-Null Count': df.count(),
                'Null Count': df.isnull().sum(),
                'Unique Values': df.nunique()
            })
            st.dataframe(col_info, use_container_width=True)
        with tab2:
            st.subheader("Missing Data Analysis")
            missing = df.isnull().sum()
            missing = missing[missing>0].sort_values(ascending=False)
            if len(missing)>0:
                fig = px.bar(x=missing.values, y=missing.index, orientation='h',
                             title="Missing Values by Column",
                             labels={'x':'Number of Missing Values','y':'Column'})
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.success("No missing data found! üéâ")
        with tab3:
            st.subheader("Correlation Analysis")
            numeric_df = df.select_dtypes(include=['float64','int64'])
            if len(numeric_df.columns)>1:
                corr = numeric_df.corr()
                fig = px.imshow(corr, text_auto=True, aspect="auto",
                                title="Correlation Heatmap", color_continuous_scale='RdBu')
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("Not enough numeric columns for correlation analysis.")
    else:
        st.warning("Please upload a dataset first.")

# ---- AI Assistant ----
elif page == "AI Assistant":
    st.header("üß¨ AI Assistant")
    api_key = get_openai_api_key()
    if st.session_state.data_loaded and api_key and st.session_state.retriever and st.session_state.llm:
        st.info("üí° Ask questions about your data. I can compute correlation matrices, per-department correlations, conditional averages, quick insights, and trends.")
        for m in st.session_state.chat_history:
            with st.chat_message(m["role"]): st.write(m["content"])
        user_question = st.chat_input("Ask a question about your data...")
        if user_question:
            st.session_state.chat_history.append({"role":"user","content":user_question})
            with st.chat_message("user"): st.write(user_question)
            try:
                handled, msg = handle_analytics_query(user_question, st.session_state.df)
            except NameError:
                handled, msg = (False, "")
            if handled:
                with st.chat_message("assistant"): st.write(msg)
                st.session_state.chat_history.append({"role":"assistant","content":msg})
            else:
                with st.chat_message("assistant"):
                    with st.spinner("Analyzing your data..."):
                        answer, src = answer_with_rag(user_question)
                        st.write(answer)
                        if src:
                            with st.expander("üìö View Source Context"):
                                for i,doc in enumerate(src):
                                    st.markdown(f"**Source {i+1}:**")
                                    st.text(getattr(doc,"page_content",str(doc))[:300] + "...")
                        st.session_state.chat_history.append({"role":"assistant","content":answer})
        c1,c2 = st.columns([6,1])
        with c2:
            if st.button("üóëÔ∏è Clear Chat"):
                st.session_state.chat_history=[]; st.rerun()
    else:
        if not api_key:
            st.warning("‚ö†Ô∏è Please enter your OpenAI API Key in the sidebar.")
        elif not st.session_state.data_loaded:
            st.warning("‚ö†Ô∏è Please upload a dataset first.")
        else:
            st.warning("‚ö†Ô∏è AI system is not ready. Please reload the page.")

# ---- Visualizations ----
elif page == "Visualizations":
    st.header("üìä Data Visualizations")
    if st.session_state.data_loaded:
        df = st.session_state.df
        viz_type = st.selectbox("Select Visualization Type:",
                                ["Line Chart","Bar Chart","Scatter Plot","Pie Chart","Histogram","Box Plot"])
        c1,c2 = st.columns(2)
        with c1:
            numeric_cols = df.select_dtypes(include=['float64','int64']).columns.tolist()
            categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
            if viz_type in ["Line Chart","Bar Chart"]:
                x_col = st.selectbox("Select X-axis:", df.columns.tolist())
                y_col = st.selectbox("Select Y-axis:", numeric_cols) if numeric_cols else None
            elif viz_type=="Scatter Plot":
                x_col = st.selectbox("Select X-axis:", numeric_cols)
                y_col = st.selectbox("Select Y-axis:", numeric_cols)
            elif viz_type=="Pie Chart":
                x_col = st.selectbox("Select Category:", categorical_cols if categorical_cols else df.columns.tolist())
                y_col = st.selectbox("Select Value:", numeric_cols) if numeric_cols else None
            elif viz_type in ["Histogram","Box Plot"]:
                x_col = st.selectbox("Select Column:", numeric_cols if numeric_cols else df.columns.tolist())
                y_col = None
        with c2:
            color_col = st.selectbox("Color by (optional):", ["None"] + df.columns.tolist())
            color_col = None if color_col=="None" else color_col
        st.subheader(f"{viz_type} Visualization")
        try:
            if viz_type=="Line Chart" and y_col:
                fig = px.line(df, x=x_col, y=y_col, color=color_col, title=f"{y_col} over {x_col}")
            elif viz_type=="Bar Chart" and y_col:
                fig = px.bar(df, x=x_col, y=y_col, color=color_col, title=f"{y_col} by {x_col}")
            elif viz_type=="Scatter Plot":
                fig = px.scatter(df, x=x_col, y=y_col, color=color_col, title=f"{y_col} vs {x_col}")
            elif viz_type=="Pie Chart" and y_col:
                fig = px.pie(df, names=x_col, values=y_col, title=f"Distribution of {y_col}")
            elif viz_type=="Histogram":
                fig = px.histogram(df, x=x_col, color=color_col, title=f"Distribution of {x_col}")
            elif viz_type=="Box Plot":
                fig = px.box(df, y=x_col, color=color_col, title=f"Box Plot of {x_col}")
            st.plotly_chart(fig, use_container_width=True)
        except Exception as e:
            st.error(f"Error generating visualization: {e}")
    else:
        st.warning("Please upload a dataset first.")

# =========================
# Footer
# =========================
st.divider()
st.markdown("""
<div style='text-align: center; color: #666;'>
    <p>Grothko Consulting - Percipient Finance</p>
</div>
""", unsafe_allow_html=True)
